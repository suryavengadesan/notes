https://arxiv.org/pdf/2201.11182.pdf

Bayesian Optimization for Hyperparameter Search

H Function
	- expected maximizaiton
	- maximum probability of maximization
	- upper confidence bound
	- entropy search 

Bayesian Optimization Process 

Multi-objective optimization problem 

Genetic Algorithm 
	- define genes 
	- define selection process 
	- evaluate genes in trials 
	- perform cross over and mutation

Example
	- Genes = Hyperparameters
		- gamma, alpha, number of neurons 
		- activiation functions, elpsion, number of layers
	- Evaluation = reward from 100 steps in gym
	- Selection = random distribution across sucessfull parents 
	- Cross Over = swap hyperparameters 
	- Mutation = replace hyperparameter with random value 

Challenges
	- Agents trained on different RL models (e.g. DDPG vs. ACKTR)
		- have different types and number of hyperparameters 
	
Software

- 3 parts 
	- collection of genetic algorithm functions
	- benchmark gym environments
	- benchmark deep RL algorithms 

- Implementation
	- Submit jobs from single head node 

- Hardware
	- Intel i7-9750H CPU 
	- Nvidia GeForce RTX 2070 


	- mpi4py 
		- 



References
Codebase: https://github.com/esnet/hps-rl 

