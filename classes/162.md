# Lecture 3 

- Thread 
	- Execution Context 
		- Fully describes program state 
		- Programm counter, registers, execution flags, stack 
- Address Space (w/o translation)
	- Set of memory addresses accessible to program 
	- May be distinct from memory space of the physical machine 
- Process 
	- Instance of running a program
	- Protected address space + 1 or more threads 
- Dual Mode Operation + Protection 
	- Only systm has ability to access certain resources 
	- Combined with translation, isolates programs with each other and the OS from programs 

- Illusion of Multiple Processors
	- Programmer's View
		- Multiple CPUs to Shared Memory

- Threads are virtual cores 
- Multiple threads => multiplex hardware in time 
- Thread is executing on a processor when reisdent in processor's registers

- Each virtual core has: 
	- Program counter, stack pointer 
	- Registers

- Where is it? 
	- On real physical core 
	- Save in memory 
		- Thread control block 

- Virtual Address Space 

- Address Space 
	- Set of accessible addresses + associated state 
		- 32 bits => 2^32 = 4 billion addresses
		- 64 bits => 2^64 = 18 quintillion 

- Virtual Address Space 
	- Processor's view of memory 
		- AD is independent of physical storage 
	- Translation through page table
		- Page table is stored in operation table  

- Process 
	- protected environment with one or more thread 

- PintOS only has 1 thread 

- Process runs programs 
	- protected from each other 
	- OS protected from them 

- Modern OS
	- Anything outside the kernel is a process 

- Motivation for threads 
	- Device drivers get added in kernel mode 
	- Need to MTAO (do multiple things at once)
		- Networked servers 
		- Parallel programs 
		- Programs with user interface 
		- Network with disk bound programs 

- Threads are unit of concurrency 
	- Each thread represents one task

- Multiprocessing vs. Multiprogramming 
	- Multiprocessing
		- multiple CPUS
	- Multiprogramming 
		- multiple jobs and processes 
	- Multithreading 
		- multiple threads and processes 

- Concurrent Threads 
	- Schedule runs threads in any order and interleaving 
	- Thread may run to completion 
		- Time-slice in big chunks or small chunks

- Need to design for correctness

- Concurrency is not Parallelism 
	- Parallelism => Doing multiple things at once (MTAO)
	- Concurrency => Doing mulitple things simultaneously

- Two threads on single core system
	- Run two threads on the same core 

- Create thread 
	- Spawn new thread running given procedure 

- Jeff Dean's "Numbers Everyone Should Know" 
	- L1 Cache Reference
	- Branch Mispredict 
	- ....
	- Disk seek 
		- Handle I/O in separate thread to avoid blocking progress

- Threads Mask I/O Latency 
	- 3 States 
		- Running
		- Ready 
		- Blocked 

- System Call Interface 
	- Splits User (User Mode) from System (Kernel Mode)
	- OS Libraries perform system calls 

# Lecture 5 - IPC, Pipes, Sockets

- Goal
	- Communication between processes 
	- Pipes and Sockets
	- TCP/IP connection setup for webservers

- Fork 
	- Processes with fork()
		- copy the current process
		- state of original process duplicated in parent and child 
			- duplicates address space (memory), file descriptors, etc
	- Return value
		- >= 1 
			- running parent process
				- value is pid of new child
		- == 0 
			- running new child process
		- <= -1
			- error!
			- running original process 
	- Why Fork? 
		- cannot create new processes without it
		- original mechanism for creating concurrency in unix 
			- linux clone() provides more flexibility 

- File 
	- everything is a file
		- allows for uniformity 
		- file operations, device I/O, interprocess communication, RW, close 
		- allows simple composition of programs
			- find | grep | wc ...
	- open before use
		- opportunity for access control and arbitration 
	- byte-oriented
		- address is in bytes
	- kernel buffered reads
		- streaming and block devices looks same 
		- read blocks yielding processor to other task 
	- kernel buffered writes
		- completion of out-going transfer decoupled from application
		- allowing it will continue 
	- explicit close 

- Web Server

- C High-level File API - Streams
	- operates on streams
		- unformatted sequence of butes 
	- open stream represented by pointer to FILE data structure 
		- error reported by returning a NULL pointer
		- pointer used in subsequence operations on the stream 
		- data buffered in user space 


- Conclusion
	- IPC => communication between protected environments
	- Pipes => abstraction of single queue 
		- one end write-only
		- other end read-only
	- Sockets => abstracts of two queues
	- Pipes & Sockets 
		- Support R/W system calls similar to File I/O



# Lecture 6 - Concurrency and Mutual Exclusion

- IPC Host 
	- create communication between distinct processes

- POSIX/Unix PIPE 
	- Memory Buffer is finite 

- Socket endpoint for communication 

- Mutliplexing Processes: The process control block 
	- Kernel represents each process as as PCB
		- Process control block 
			- Status
			- Register State
			- Process ID
			- Execution time 
			- Memory space, translation
	- Kernel Scheduler 
		- maintains data structure of PCB
			- gives CPU to different processes
			- uses a policy decision
	- Give out non-CPU resources
		- Memory/IO 
		- Another policy decision 

- Context Switch 

- Lifecycle of Process or Thread
	- Ready -> Running -> Waiting
	- New -> Terminated 

- Scheduling: All about queues 
	- PCB moves from queue to queue 
	- Separate queue for each device, signal, condition

- Ready queue and varios I/O device queues 
	- process not running -> pcb is scheduler queue 
	- each queue has different scheduler policy 

- single and multithreaded processes 
	- threads (active component)
	- address space (passive component)
		- prevents buggy programs from thrashing system 

- shared vs. per-thread state 
	- shared state
		- heap
		- global variables
		- code
	- per-thread state
		- TCB 
			- stack info, saved registers, thread metadata
		- Stack 
	- per-thread stated

- core concurrency - dispatch loop 
	- loop 
		- run thread 
		- choose next thread
		- save state of cpu w/ curr TCB 
		- load state of cpu w/ new TCB

- run thread
	- load its state (registers, PC, stack pointer) into CPU
	- load environment (virtual memory space, etc)
	- jumpt to pc

- dispatcher get controls
	- internal events -> thread returns control voluntarily
	- external events -> thread gets preempted 

- internal events
	- blocking I/O 
		- requesting I/O implicitly yields CPU
	- waiting signal from thread 
		- thread asks ot wait and yeild CPU 
	- thread executes yield()
		- thread volunteers to give up CPU 

- thread API 
	- pthread_create
	- pthread_exit
	- pthread_join

- stack for yielding thread

- threads switch between kernal and user stack 

- TCB + stacks -> contain complete restartable state of thread 

- Threading models
	- One-to-one threading
		- 1 kernel to 1 user 
	- Many-to-one threading
		- 1 kernel to many user
	
- Process vs. Threads
	- Switch overhead
	- Protection
	- Sharing Overhead
	- Parallelism 

- Simultaneous Multithreading/Hyperthreading
	- Hardware scheduling technique
		- superscalar can execute multiple independent instructions
		- hyperthreading duplicates register state
	- can scheduler each thread as if separate CPU 

- Thread blocks I/O
	- Thread requires data block from filesystem 
		- syscall
		- read operation
		- run new thread/switch 
	- Thread communication is similar 
		- wait for signal/join + networking
	
- Interrupt Controller 
	- chooses which interrupt to honor
		- identity specificied with ID line
		- mask enables/distables interrupts
		- priority encoder picks highest enabled interrupt
		- software interrupt set/cleared by software
	- CPU disable all interrupts with internal flag
	- Non-maskable interupt line (NMI) can't be disabled

- Example: Network interrupt
	- pipeline flush 

- TCB and Stack Initializiation
	- Initialize register fields of TCB 
	- Initialize stack data

- Correctness with Concurrent Threads
	- non-determinism
		- scheduler can run threads in any order
		- scheduelr can switch threads at any time
	- independent threads
		- no state shared with other threads
		- deterministic, reproducible conditions
	- cooperating threads
		- shared state between multiple threads
	- goal: correctness by design 

- Example: ATM Bank Server

- Concurrency is challenging
	- difficult for practicing engineers
		- challenge to debug 

# Lecutre 10 - Scheduling 1: Concepts and Classic Policies

- Monitor and Condition Variables
	- Monitor - lock with 0>= condition variables
		- programming paradigm (e.g. Java)
	- Condition Variable - queue of threads waiting inside critical section
	- Operations
		- Wait
		- Signal
		- Broadcast
	
- Layers of I/O

- Different Types of I/O
	- Process Management
	- Memory Management
	- Filesystems
	- Device Control
	- Networking

- Internal OS File Descriptor
	- Internal Data Structure of file 
		- Where it resides
		- Its status
		- How to access it

- Device Driver
	- Device-specific code in kernel 
	- Two Pieces
		- Top Half
		- Bottomg Half 

- Life Cycle of I/O Request
	- User Progam
	- Kernel  I/O Subsystem
	- Device Driver Top Half
	- Device Driver Bottom Half
	- Device Hardware

- Scheduling
	- Which thread runs on CPU next 
	- Many goals, policies, and schedulers

- CPU Bursts 
	- Programs burst between CPU and I/O 

- Scheduling Policy Goals/Criteria
	- Minimize response time
	- Maximize throughput
	- Fairness

- Gantt Chart

- First-come, First-served (FCFC) Scheduling
	- i.e. FIFO, Run until done
	- Convoy Effect -> short process stuck behind long process

- Round Robin (RR) Scheduling
	- Each process gets one time quantum
	- Scheduled back to end of queue

- SAD ASF 

# Lecture 23 

## Networking Layering 

- Layering
	- Complex services out of simplers ones 
	- Physical link layer is limited 
		- Maximum Transfer Unit = packet size of 100-1500 bytes 
		- Routing limited to physical link wire with switch 
	
## UDP Transport Protocol 
- IP Protocol 17 
	- IP Header 
		- Adds minimal header to deliver from process to process
	- IP Source Port 
	- UDP Data 

- Lower Overhead 
	- Used for high bandwitch message (images, videos)

# Lecture 24 

- Distributed Consensus Making 
	
- Consensus Problem
	- All nodes propose a value 
	- Seom nodes might crash and stop responding 
	- All remaining nodes decide on some value 
- Distriubted Decision Making 
	- Choose between true and false 
	- Choose between commit and abort 
- Two Phase Commit Protocol 
	- Persistent Stable log on each machine 
	- Prepare Phase
	- Commit Phase 

- Network Protcols 
	- Many Levels 
		- Physical Level -> mechanical and electrical network 
			- Ethernet, Wifi, LTE
		- Link Level -> packets formats/error control
		 	- Ethernet, Wifi, LTE
		- Network Level -> network routing, addressing
			- IP 
		- Transport Level -> reliable message delivery
			- UDP TCP 
		- Application Level 
			- RPC, NFS, WWW, E-mail, SSH 

- Layering 
	- build complex services from simpler ones 
	- each layer provides services needed by many higher layers by utilizing services provided by lower layers 

- Physical link layer is limited 
	- Packets are of limited size
		- Maximum Transfer Unit (MTU)
			- Packets of limited size 
	- Routing in limited with physical link 
	- Construct secure, ordered, message service routed to anywhere 

- Physical Reality (Packets)
	- MTU -> Limited size 
	- Unordered (sometimes)
	- Unreliable
	- Machine-to-Machine 
	- Only on local area net 
	- Asynchronous 
	- Insecure 
- Abstractions (Messages)
	- Arbitrary Size 
	- Ordered 
	- Reliable 
	- Process-to-process
	- Routed anywhere 
	- Synchronous 
	- Secure 

- IPV4 Packet Format 
	- Wrapper of data 
	- Can use IP format to send data 
		- from one machine to same machine 
		- frome one machine to another machine 

- Internet Architecture 
	- Five Layers 
		- Lower three layers implemented everywhere
		- Top two layers implemented only at hosts 
	- Host A, Router, Host B 

- Layering Analogy
	- Packets in envelopes

- Internet Transport Protocols 
	- Datagram service (UDP) - IP Protocol 17 
		- No-frills extension of best-effort IP 
		- Multiplexing and demultiplexing among processes 
	- Reliable, in-order deliver (TCP) - IP Protocol 6
		- Connection set-up & tear-down 
		- Discarding corrupted packets (segments)
		- Retransmission of lost packets (segments)
		- Flow control 
		- Congestion control 
	- Other Examples 
		- DCCP (33) - datagram congestion control protocol 
		- RDP (26) - reliable data protocol 
		- SCTP (132) - stream control transmission protocol

- Sockets in concept 
	- Client
		- Create client socket 
		- Connect it to server (host:port)
		- Connectoin Socket -> Connection Socket
		- Close client socket 
	- Server
		- Create Server Socket 
		- Bind it to an address 
		- Listen for connection 
		- Accept syscall()
		- Close connection socket 
		- Close server socket

- Reliable Message Delivery - The Problem 
	- All physical networks can garble or drop packets
	- Physical media -> packet not transmitted received
		- If transmit close to maximum rate
		- If transmit at lowest voltage such that error connection just starts correct errors
	- Congestion -> no place to put incoming packets
		- Point-to-point network -> insufficient queue at switch/router
		- Broadcast link -> two host try to use same link 
		- In any network -> insufficient buffer space at destination 
		- Rate mismatch -> what is sender send faster than receiver can process 

- TCP (Transmission Control Protocol) - IP Protocol 6
	- Stream in -> Router -> Stream out
	- Reliable byte stream between two processes on different machine
		- Over internet (read, write, flush)

- TCP Details 
	- Fragments byte stream into packets 
		- Hand packets to IP 
		- IP may also fragment by itself 
	- Uses window-based acknowledgement protocol 
		- Window reflects storage at receiver
		- Window shoudl reflect speed capacity of network 

- Dropped Packets
	- Physical hardware problems (bad wire, bad signal)

- Ensure transmission of packets
	- Send packet, send back ACK message
		- Detects garbling via check sum 
	- Timeout and retransmission

- Stop-and-wait (No packet loss)
	- Send, wait for ACK, repeat 
	- RRT (Round Trip Time)
		- Time takes packet to travel from sender to receiver and back 
	- For symmetric latency 
		- RTT = 2d

- Stop-and-wait (No Packet Loss)
	- Little's Law applied to the network
	- Loss recovery relies on timeouts 

- Stop-and-wait (With Packet Loss)
	- Loss recovery relies on timeouts 

- Deal with Message Duplication

- Advantage of Moving Away from Stop-and-wait 
	- Layer space of acknowledgements
		- Pipelineling: Don't wakt for ACK before sending 
	- ACKs serve dual prupose 
		- Reliability -> confirm packet recieved
		- Ordering -> Packets can be reorderd at destination 
	- Compute data in flight

- Remote Procedure Call (RPC)
	- Raw messaging is a bit too low-level for programming 

- RPC Concept 
	- Client(Caller) -> call -> Client Stub -> send 
	- Server(Callee) <- return <- Server Stub <-receive

- Client Stub -> Packet Handler -> Network -> Packet Handler -> Server Stub 

- RPC Implementation 
	- Stub provides glue on client and server
		- Client stub responsiblef or marshalling arguments and unmarshalling return values

- RPC Details 
	- Equivalence with regular procedure call 
		- Parameters -> request message
		- Resut -> reply message
		- Name of procedure -> passed in request message
		- Return address -> mbox2
	- Cross-platform issues 
	- Binding -> process of converting a user-visible name into network endpoint 
		- Static -> fixed at compile time 
		- Dynamic -> performed at run time 
	- Dynamic Binding 
		- RPC systems determines binding from name service 
		- Benefits
			- Access Control -> check who permitted access service
			- Fail-over -> if server fails, use different one
	- Multiple Servers 
		- Give flexibility of binding time 
	- Non-atomic Failures 
		- different failure modes on single machine 
			- user level bug 
			- macihine failure 
		- Inconsistent view of word
			- Did cache get written back?
			- Did server perform request or not?
		- Solution 
			- Distributed transaction 
			- Byzantine Commit 
	- RPC Systems
		- CORBA
		- DCOM
		- RMI 
- Microkernel operating systems 
	- Monolithic vs. microkernel 
	- Benefits
		- Fault isolation -> Bugs are isolated
		- Enforces modularity 
- Network-attached storage
	- CAP Theorem by Eric Brewer
		- Consistency -> changes appear to everyone in same serial order
		- Availability -> get results at any time 
		- Partition-Tolerance -> system continue to work when network become partitioned
	- Cannot have all three at same time 

- Summary 
	- TCP 
	- RPC 
	- Distributed File System
	- Cache Consistency 

# Lecture 25: Distributed Storage, NFS and AFS, Key Value Stores

- Recall 
	- RPC Information Flow
	- CAP Theorem 

- Distributed File Systems 
	- Model
		- Client -> Read File -> Server 
		- Client <- Data Sent <- Server 
	- Mount remote files into local file system 
	- Naming Choices
		- Tuple Name => (Hostname, localname)
		- Global name space => unique global filename (with hashing)

- VFS -> Virtual File System 
	- allow for multiple filesystem types on different networks

- Layers of I/O
	- Device Driver

- Virtual Filesystem Switch
	- VFS -> virtual abstraction similar to local file system 
	- Provide virtual superblocks, inodes, files
	- Compatible with local and remote filesystems

- VFS Common File Model in Linux 
	- Primary Object Types 
		- superblock object
		- inode object 
		- dentry object 
		- file object

- Simple Distributed File System
	- Read RPC, Return Data
	- Remote disk => reads and writes forwarded to server
	- Use RPC to translate system into remote requests
	- Advantage
		- Server provides consistent view of file system to multiple clients
	- Challenges
		- Slow performance
	
- Caching
	- Add caches to reduce network load
		- read(f1)
		- write(f1)
	- Advantage
		- Operations can be done locally
	- Challenges
		- Cache data not commited during failures
		- Caches not consistent

- Dealing with Failures
	- Server crashes

- Stateless Protocol 
	- Request contains information requred to service
		- e.g. HTTP, NFS
- Idempotent Operations 
	- Multiple executions equals to one operation

- Case Study: NFS (Network File System)
	- Three Layers
		- UNIX file-system interface
			- open, read, write, close calls + file descriptors 
		- VFS layer
			- distinguishes local from remote files
		- NFS service layer
			- bottom layer of architecture
	- NFS Protocol 
		- RPC for file operation on server
	- Write-through caching


# Resouces  
- Spring 2020 Lectures
	- https://www.youtube.com/@johnkubiatowicz3737/videos 

# Operating Systems - Principles and Practice

# Volume 1 - Kernels and Processes

OS => (1) referee (2) illusionist (3) glue 

## ChapteKernel Abstraction => 

process => execution of application with restricted rights 
	=> protection is provided by os kernel 

process abstraction
	=> program executuble => compiler's output machine code and data 
	=> execution stack => holds local variables during procedure calls 
	=> heap => memory to dynamically allocate data structures for program 

OS execution 
	=> set stack pointer
	=> jump into first instruction 

Analogy
	process => program
	object => class 

Process Control Block 
	=> data structure that keeps track of various processes 
	=> contains information about process 
		-> where it is stored in memory
		-> where executuble image is stored in disk 
		-> which user asked to execute 
		-> privileges of process 
		-> etc 

program consistes of concurrent activites => threads 
	- each thread has separate program counter and stack 
	- process = multiple threads 

dual operation mode 
	- 

## Chapter 3 - Programming Interface

### 3.1 - Process Management 

- Shell -> job control system 

### 3.2 - Input/Output


# Volume 2 - Concurrency

## Chapter 5 - Synchronizing Access to Shared Objects
- Cooperating thresds -> read and write shared state

- Reasons single thread and multithreading differ
	- interleaving of thread access to shared state
	- program execution can be nondeterministic
		- heisenbugs (nondeterministic)
		- bohrbugs (deterministic)
	- compiler and processor reorder operations
		- optimizations are performed to preserve single threaeded operations

- Solution
	- extend object oriented programming to multi-threaded programs

- Shared Objects
	- accessed safely by multiple threads
		- variables on heap, static variables, global variables

- Monitor
	- shared object supported by a programming lanaguage


- Implemented Shared Objects
	- Shared object Layer
	- Synchronization variable layer
	- Attmoic instruction layer 

- Synchronization variable
	- data structure for concurrent access to shared state

- State variable
	- normal varaibles for objects (e.g. int, str, arr, pointer)

- Shared Objects
	- Built with two synchronization variables
		- Locks
		- Condition Variables



## Chapter 7 - Scheduling 

- Scheduling Conditions: 
	- Threads > Number of Processors 

- Task -> user request 
- Response Time -> user perceived time to complete task 
- Starvation -> lack of progress for task

## 7.1 - Uniprocessor Sheduling 
- Simple Policies
	- FIFO
	- SJF 
	- RR 

- Resource Allocatin System
	- Processors
	- Memory
	- Network
	- Disk 

- Workload => Set of tasks

- Preempt (stop task)
	- Timer Interrupt
	- Higher priority task 

## FIFO

## SJF

- Shortest job first - always schedule task with shortest remaining time 
	- Assumes each task time is known
		- Approximated alogirthms exist
	- Minimizes average task time 

## Round Robin

- tasks take turns running on processor for limited time period
- 

## Max-Min Fairness

- Simply fairness
	- prses oviding fairness among proces(generalizes to users, applications, threads)

- Max min fairness
	- iteratively maximizes minimum allocation given to particular process
		- repeats until all resources are assigned

- Simplication
	- all processes are compute-bound
		- max min fairness simplifies to round robin 

## Multi-Level Feedback Queue
- Scheduling algorithm meetimg multiple goals: 
	- Responsiveness
	- Low Overhead
	- Starvation-Freedom
	- Background Tasks
	- Fairness

# 7.2 Multiprocessor Scheduling

## Scheduling Sequential Applications
- MFQ Bottlenecks
	- Contention for MFQ locks
	- Cache Coherence Overhead
	- Limited cache reuse 

## Scheduling Parallel Applications
- Oblivious Scheduling
	- each thread scheduled as independent entity
	- bottlenecks
		- bulk synchronous delay
		- producer-consumer delay
		- critical path delay
		- pre-emption of lock holder
		- I/O
	
- Gang Scheduling
	- Schedule all tasks of a program together

- Space sharing
	- allocating different processors to different tasks

- Time sharing
	- allocating single processor among multiple tasks
		- alternative time each is scheduled onto processor 

# 7.3 Energy-Aware Scheduling
- Power optimizations
	- Processor design
	- Processor usage
	- I/O device power

# 7.4 Real-Time Scheduling
- Real-time constraints
	- Computation must be completed by deadline to have value
	- Value uniform until deadline, when value drops to 0

- Responsiveness -- timeliness matters without strict deadline
- Techniques to increase likelihood deadline met
	- Over-provisioning
		- Ensure aggregation of tasks need only fraction of system capacity
	- Earliest deadline first
		- Execute tasks in sorted order of deadline
	- Priority donation
		- resolves priority inversion = high priority tasks waits for lower priority task to complete
		- high prioity task donates it's priority for lower priority task to finish quickly, then recollects the priority 

# 7.5 Queueing Theory 
- Response time varies non-linearly with rate tasks arrive to system

- Assumptions
	- Work conserving
		- All tasks eventually completed
	- Assume FIFO

- Simplications
	- Single queue, single esrver, work conserving systesm

- Defintions
	- Server
		- something that performs tasks
			- e.g. cashier, waiter, computer system 
	- Queueing Delay
		- delay = wait time = time task must wait to be scheduled
	- Number of Tasks Queued
	- Service Time
		- Time to complete task assuming no waiting
	- Response Time
		- Queueing Delay + Service Time
	- Arrival Rate (lambda)
		- Average rate new tasks arrive
	- Arrival Process
		- Bursy or evenly spread over time
	- Service Rate (mu)
		- number of tasks server can complete per unit time
	- Utilization
		- Fraction of time server is busy
			- 0 <= U <= 1
		- U = lambda/mu = (arrival rate)/(service rate)
	- Throughput
		- number of tasks processed per unit of time
			- utilization * arrival rate
		- if U < 1, throughput = arrival rate
		- if U > 1, throughput = service rate 
	- Number of tasks in system
		- tasks in queue + tasks being serviced
- Little's Law
	- applied to a stable system
		- arrival rate = departure rate
	- number of tasks in system = throughput * response time
		- N = X * R
- Response Time vs. utilization
	- higher utilization => higher queueing delays => higher response times
- Evenly spaced arrivals (best case)
	- three cases
		- lambda < mu
			- no queueing delay, response time = service time
		- lambda = mu
			- if queues are empty, they remain empty
			- if queue are full, they remain full
		- lambda > mu
			- queues grow without bound 
- Bursty arrivals (worst case)
	- group of tasks arrive at once
		- wait time grows linearly with more tasks
- Exponential arrivals
- What If's
	- Scheduling Policy
		- response time curve
			- depends on burstiness and predictability of workload
		- less bursty than exponential arrival
			- FIFO is optimal
			- Round Robin is suboptimal
		- exponential arrival
			- FIFO and round robin perform the same 
		- task length can be predicted
			- SJF improves response times for busty arrivals
	- Workloads with varying Queueing Delay
	- Multiple servers
	- Secondary Bottlenecks

# 7.6 Overload Management
- analogies
	- highways and car traffic

# 7.7 Servers in Data Center
- front end receives request
	- back end balances load to nodes in the backend 

# 7.8 Future Direction
- Challenges with Resource Scheduling
	- multicore systems
	- cache affinty
	- energy-aware scheuling
	
# Volume 3 - Memory Management 

## Chapter 8 - Address Translation

- Address Translation applications
	- process isolation
	- interprocess communication
	- shared code segments
	- program initialization
	- efficient dynamic memory allocation
	- cache management
	- program debugging
	- efficient I/O
	- memory mapped files
	- virtual memory
	- checkpointing and restart
	- persistent data structures
	- process migration
	- information flow control
	- distributed shared memory

## Chapter 9 - Caching and Virtual Memory

## Chapter 10 - Advanced Memory Management

# Volume 4 - Persistent Storage 

References
https://www.kea.nu/files/textbooks/ospp/osppv1.pdf

# Operating System in Three Easy Steps

## Chapter 18 - Paging: Introduction
- Space-management problem
	- Two approaches
		- Variable-sized segmented pieces
			- used in virtual memory segmentation
			- split up process's address space in variable-sized logical segments
				- code, heap, stack
			- space becomes increasingly fragmented over time
		- Fixed-sized segmented pieces
			- used in virtual memory paging
			- split up process's address space in fixed-sized logical segments
				- page
- Physical memory => array of fixed-sized slots
	- page grames => each contain a single virtual-memory page

- Example
	- 64 byte address space
	- 4 pages 16 byte each 

- Paging advantages
	- flexibility -> support abstraction of address space

## Chapter 19: Faster Translations (TLBs)
- translation-lookaside buffer -> hardware cache of popular virtual to physical address translations
	- a.k.a. address-translation cache 


## Chapter 20: Paging: Smaller Tables

## Chapter 21: Beyond Physical Memory: Mechanisms