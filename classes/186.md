# Note 1 - SQL 1
- table, fields, alian (as)
- distinct 
- select <columns>
- from <table>
- where <predicate>
- boolean operators (found in predicate of where clauses)
    - NOT, AND, OR
- NULL values
- aggregation (Count, Sum, Avg, Max, Min)
- groupby
- having 
- orderby
    - defaults to ascending (ASC), specify descending (DESC)
- limit (number of rows)
- with - allows you to define table before query 

discussion slides
- LIKE 
    - '_' is a single character
    - '%' multiple character 
- CAST x as FLOAT

Practice Questions:
- fa20 q2 
- fa21 q2 
- sp21 q1 
- vitamin 1
- project 1 
- discussion 1

# Note 2 - SQL II  
- multitable query 
    - using cross join (cartesian product)
        - use where clause to combine rows
- inner joins (combine rows using the on join clause)
    - exact same function as cross join 
- outer joins (combine rows using the on join clause)
    - left outer join (all left rows ensured, right extra set to null)
    - right outer join (all right rows ensured, left extra set to null)
    - full outer join (all left and right rows ensured, with extra non matched pairs set to null)


- nested join 
    - perform joins from left to right

- subqueries
- correlated subsqueries
- natural join
- set operators
    - all
    - any, union, intersect, difference, in

Practice Questions:
- fa20 q2 
- fa21 q2 
- sp21 q1 
- vitamin 1

# Note 3 - disks, files, buffers

- disk api (read, write)
    - read = transfer pages of data from disk to ram
    - write = transfer pages of data from ram to disk 

- disk structure - platters, arm assembly, sectors
- SSD's (fine-grain reads and course-grain writes)

- files, pages, records
    - records (rows) stored in relations (tables)
    - smallest unit of transfer from disk to memory = page
    - each relation is stored as a file
        - records saved in pages, which are stored in the file

- database determines attributes based on access patterns and schema
    - 1 type of file (e.g. heap or sorted)
    - how pages are organized in a file
    - how records are organized in a page
    - how each record is formatted

- IO = write to disk, or read to disk 

- heap file (no ordering of pages, or no ordering of records in pages)
    - linked list implementation 
        - datapage = next and prev pointers, free space tracker, record 
        - 1 header page = splits pages into free pages and full pages 
        - free pages appended when more free space needed
        - newly full pages pushed to start of full pages section 
    - page directory implementation
        - multiple header pages
            - headerpage = pointer to next header page, datapages, spaces left in datapages
        - faster insertions
    => Read: Full Scan
    => Write: Scan only headers 

- sorted files (sorted by pages and records sorted by key)
    - page directory implementation
    => Read: O(logN)
    => Write: O(n)

- record types
    - fixed length records
    - variable length records (contain varchars)
        - contiains a header with pointers at the the end of variables 

- each record is uniquely identifiable using record id 

- record types
    - fixed length records (FLR)
    - variable length records (VLR)

- page types
    - page with FLR (packed)
        - contains page header with record count 
        - compute offsets for inserts, and shift bytes for removals 
    - page with FLR (unpacked)
        - contains page header with record count 
        - contains bitmap for unpacked pages, to keep track of open slots 
    - page with VLR (unpacked)
        - contains page footer with a slot directory 
        - tracks slot count, free space pointer, and entries 
            - each record entry = [record pointer, record length]
            - free space pointer points to next open slot 
            - slot count is a integer
    - page with VLR (packed)
        - reshift records with deletion 

Common Questions:
1. How many bytes should you change in a page? 
2. How many bytes of free space remain in a page? 
3. How many IO's to add to heap file? 
4. How many IO's to add to a sorted file? 

Practice Questions: 
- fa20 q3 
- fa21 q3 
- vitamin 2

# Note 4 - b+ trees

general properties 
- b+ tree is an type of index 
- entries within the nodes must be sorted 
- must have d <= x <= 2d elements 
- always balanced 

insertion
    - step 1: find node with valid key range, and perform insertion sort on the node 
    - step 2: handle overflows
        - if leaf node, copy into parent
        - if inner node, move into parent 
    - step 3: if parent overflows, recurse step 2 on it

deletion 
    - just delete key from the leaf node
    - never delete a key from inner node, bc it seerves a search not storage 

storing records
    - alt1 (by value - sorted and clustered by default)
        - leaves store records, not pointers to records
        - doesn't support multiple indices per tree file 
    - alt2 (by reference)
        - leaves contain record ids (pagenum, recordnum) which are pointers to the records
        - allows for multiple indices per tree file, bc records are stored separately from the tree
    - alt3 (by list of references)
        - each leaf can hold lists or pointers, instead of just one pointer
        - think project implementation

clustering 
    - clustered = keys are sorted for leaf nodes (hence don't need to repeat search)
    - unclustered = keys aren't sorted for leaf nodes (hence need to repeat search)
    - IO for unclustered and clustered is roughly the same at 1 IO per key 

counting IO's
    - find root to leaf path

bulk loading
    - add keys into right most leaf until it overflows

keys = 2d * (2d + 1)^h 

tree types
alt 1 - (by value) child are single values
alt 2 - (by reference) childs are points 
alt 3 - (by list of references) childs are pointers to lists

Practice Questions:
- fa20 q4 
- sp21 q3 
- fa21 q4 
- sp22 q3
- vitamin 3 


# Note 5 - buffer management 

- buffer pool 
    - splits memory into frames
        - each frame can store a single page
    - store metatable to track frames efficiently
        - frameid, pageid, dirty bit, pin count
            - frameid = unique to memory address
            - pageid = frame associated with a page 
            - dirty bit = if page has been modified 
            - pin count = if page has been modified

- Page Request Handling
    - hits = page is found in buffer pool
    - misses = page is not found in buffer pool, and needs to be called from disk
    - hit rate = hits/(total requests) OR hits / (hits + misses)

- LRU
    - NOTE: causes sequential flooding 
    - keep track of least recently used buffer frame for next eviction
        - page hit = page already in memory 
- MRU
    - keep track of most recently used buffer frame for next eviction
        - page hit = page already in memory 
- Clock
    - keeps track of clock hand on a frame 
        => clock hand remains in same location during hits 
        - page hit = pages already in memory 

Practice Questions: 
- fa20 q5 
- sp21 q4
- fa21 q1 
- sp22 q2 
- vitamin 4 


# Note 6 - sorting
- B = buffer size 

- two way external sorting
    - conquer phase = N (only have buffer of size of 1 page)
    - merge phase => divide sorted runs by 2

- full (general) external sorting 
- given buffer size b 
    - conquer phase = (N/B)
    - merging phase => divide sorted runs by B-1 

- total IO 
    - 2N * (1 + ceil(log_(b-1) ceil(N/B))) s.t. N = number of pages
    
Common Questions: 
1. What is the IO for sorting this data? 

Practice Questions:
- sp21 mt1 5a
- vitamin 4


# Note 7 - hashing
- B = buffer size
- external hashing
    - partitioning phase (divide) = split into B - 1 partitions 
    - hashing phase (conquer)
        - build & probe -> if current parition size <= B, save to hash table 
        - recursive hashing -> if any partition >= B 
- note: Double check passes (both read & write)! 
     
- total IO => NEVER APPLY SORT FORMULA 
    - R for input, W for output, and RW for each intermediate partitioning phase 
    - (sum_(i=1)^(m) r_i + w_i) + 2X
- count IOs (1) partition phase IO's (2) build and probe phase IOS

- B = N/P pages per partision s.t. N = total pages, P = total partitions, B = pages per partition

Common Questions: 
1. What is the IO for hasing this data? 

Practice Questions:
- sp21 mt1 5b
- vitamin 5 


# Note 8 - joins

join = merging two relations (tables) into new records based on join condition 

writing to disk is not included in join cost model 

Outer Relation = R 
Innter Relation = S 

Single Nested Loop join - SNLJ
IO Cost: [R] + |R|[S] s.t. [X] = pages in table X, |X| = records in table X
    => Check every record of R against every record of S 

Page Nested Look join - PNLJ 
IO Cost: [R] + [R][S]
    => Check every page of R against every record of S, do this for every page in R 

Block Nested Loop join - BNLJ
IO Cost: [R] + ceil([R]/B-2)*[S]
    => Check every B-2 page block of R against every record of S 

Index Nested Loop Join - INLJ - equijoin
IO Cost: [R] + |R|*(cost of look up matching record in S)
    => Used if index exists on join condition 

Sort Merge Join - SMJ - equijoin
IO Cost: sort S + sort R + [S] + [R]
*Sort Cost: 2N * (1 + ceil(log_(b-1) ceil(N/B))) s.t. N = total number of pages, B = buffer pages 

Optimizations(Refinement) 
Note: runs(X) => total runs from last sort step 
(1) Runs(R) + Runs(S) <= B-1 => Save 2 * ([R] + [S])
    => If can allocate page for each run of [R] and each run of [S]
    => Allows to combine last sorting and merge phase
(2) Runs(R) + 1 <= B - 1 => Save 2 * [R]
(3) Runs(S) + 1 <= B - 1 => Save 2 * [S]
 
Naive Hash Join
IO Cost: [R] + [S]

Grace Hash join - GHJ - equijoin
IO Cost: Cost of hashing + Cost of Naive Hash Join of Subsections
Note: Keep partitioning until <= B-2 

Common Questions: 
1. What is the minimum IO for performing the join operation? 
2. What is the IO after optimizing the join operation? 

Practice Questions: 
Sp81 MT2 Q5  

# Note 9 - relational algebra

relational algebra => procedural programming language 
    => take in a relation and output a relation 

Projection (pi) 

Selection (sigma)

Union (u)

Set Difference (-)

Intersection (n)

Cross Product (x)

Joins (bowtie)

Rename (rho)

Group By/Aggregation (gamma)

Common Questions: 
1. Does this relational algebra satisfy this query output? 


# Note 10 - query optimizer

Query Optimization => find query plan that minimizes IOs 
    => sequence of operations to execute query, represented as relational algebra 
    

Iterator Interface
    - Query plan => Expression Tree
        => Leaf = relations (tables)
        => Inner Nodes = operators (relational algebra)
        => Edges = flows of records from operator 
        => Vertex => table access operator 
    
    - Query Optimizer creates Sequence of operators
        - Query executor creates Iterators for each operator 
        - Iterator forwards tuples to next operator 

    - Next() performs algorithm on operator instance
        - Streaming "on-the-fly" (little compute like select)
        - Blocking "batch" (consume entire input like sort)

    - Single Threaded
        - Recursively call next until reach base operator 

Selectivity Estimation
    - Use selectivity values appendix 

Compute pages or records from query 
    - Compute selectivity 
    - Multiply total number of pages from query without predicates to selectivity score 

Selectivity of Joins
    - Join's Selectivity (JS) = (|A| * |B|)/(max(uniqueValue(A), uniqueValues(B))) s.t. A, B are tables to Join
    - Estimated Number of Joined Tuples (EJT) = JS * |Tuples in Cartesion Product of A and B|
    - Estimated Number of Pages = EJT / (Estimated number of joined typles per page )

Common Heuristics
    1. Push down projects and selects
    2. Only consider left deep plans
    3. Don't consider cross joins (unless only option)

System R
SR - Pass 1 => only considers single tables or joins from single table 
    - IO => Two Access Types 
        1. Full Scan IO
            => [P] I/O's s.t. table P
        2. Index Scan IO 
            => Alt 1 Index 
                => (cost to reach leaf) + (num leaves read)
                    => clustered: (inner leaf) + (selectivity * records)/(records/page)
                    => unclustered: (inner leaf) + ((selectivity * records)/(records/page) + (selectivity * records))
    - Valid Access Plans to Advance
        1. Optimal Plan (for each relation/table) = lowest IOs 
        2. Plan with interesting order
            (i) Order By 
            (ii) Group By
            (iii ) Any down stream join (no full scans)

SR - Pass 2 to N
    - Valid Access Plans to Advance
        1. Optimal Plan = lowest IOs 
        2. Plan for each set of interesting orders 
            (1) SMJ must be the last join of the set 
                (i) SNLJ, INLJ on left relation (odering preserved)
            (2) NEVER GHJ, PNLJ, BNLJ 
    
Counting Join I/Os 
    - Considerations
        1. if we materialize or stream the intermediate relations
        2. if previous operator's interesting orders reduce IO 
    
    Consideration 1
        - Materializing intermediates involve read and write to disk IOs 
        - Streaming intermediates involves no IOs 
        - System R never materializes*

    Consideration 2
        - Case by case basis


Selectivity Appendix
(1) Equalities
    c = v ->  1/|c| or 1/10
    c_1 = c_2 -> 1/max(|c_1|, |c_2|) or 1/|c_i| or 1/10
(2) Inequalities Integers
    c < v -> (v - min(c)) / (max(c) - min(c) + 1) or 1/10
    c > v -> (max(c) - v) / (max(c) - min(c) + 1) or 1/10
    c <= v-> (v - min(c)) / (max(c) - min(c) + 1) + 1/|c| or 1/10
    c >= v-> (max(c) - v) / (max(c) - min(c) + 1) + 1/|c| or 1/10
(3) Inequalities Floats 
    c <= v-> (v - min(c)) / (max(c) - min(c))
    c >= v-> (max(c) - v) / (max(c) - min(c))
(4) Connectives
    AND -> S(p1) * S(p2)
    OR  -> S(p1) + S(p2) - S(p1 AND p2)
    NOT -> 1 - S(p)
Note: Connective formulas apply to different indices 

Common Questions: 
1. Estimate full scan IO? 
2. Estimate index scan IO? 
3. Estimate IO for pass 1 of system R? 
4. Estimate IO for pass n of system R? 
4. Find valid plans after pass 1? 

# Note 11 - Transaction and Concurrency I 

Problems with concurrent Access 
    (1) Inconsistent Reads (Write-Read Conflict)
        => partial read of updated value 
    (2) Lost update (Write-write Conflict)
        => one of the two updates gets lost 
    (3) Dirty Reads (Write-Read Confict)
        => reads update that is never commited
    (4) Unrepeatable reads (Read-Write Conflict)
        => update occurs inbetween two reads

Transaction => sequence of multiple actions execute in single, logical, atomic unit 
    => gaurantee ACID Properties 

Atomiticity => either commits or aborts 
Consistency => db starts and ends consistent during transaction 
Isolation => execution of transaction isolated from others, although interleaved under the hood 
Durability => transaction commits persists despite failures 

Concurreny Control => Isolation enforced in transaction schedules 

Equivalent Schedules 
(1) Involve same transactions
(2) Operations ordered same way
(3) Leave database in same state 

serializable (view serializable (conflict serializable (serial)))

Serial Schedule => every transaction runs without interleaving 

Serializable => schedule equivalent to serial schedule 

Conflicting Operations 
(1) operations from different transactions 
(2) operations operate on same resource 
(3) one operation is write 

Conflict Equivalent => schedules with conflicts in same order 

Conflict Equivalent ot Serial => Performing topological sort
    -> Repeatedly remove nodes in graph without incoming edges, removing outgoing edges in the process 

Conflict Serializability => schedule conflict equivalent to serializable schedule 
    - conflict serializable implies serializable 

Conflict Dependency Graph => acyclic graph <=> conflict serializable
(1) 1 node per Xact 
(2) Edge from T_i to T_j if 
    (2i) operation O_i of T_i conflicts with O_j of T_j
    (2ii) O_i appears earlier in schedule than O_j 


View Serializability 
- S1 and S2 view equivalent
    (1) same initial reads
    (2) same dependent reads
    (3) same winning reads 
- Blind Write => sequence writes with no interleaving reads 
- Conflict Serializable subset of View serializiable 
- Conflict Serializable + Blind Writes = View Serializable 

Predicate Locks => apply lock on a range of records, prevents phantom problem

Common Questions: 
1. Determine if transaction is conflict serializable? 
2. True False of ACID properities 

# Note 12 - Transaction and Concurrency II 

Isolation => each transaction is isolated by itself 

DBMS interleaves transacations while gauranteeing isolation, as if transacations ran one after the other

Lock => grants read and write permissions of transacations
X lock => exclusive write 
S lock => shared read 

Two Phase Locking => ensures conflict serializable schedules 
    (1) transacations require S lock before reading, and X lock before writing 
    (2) cannot acquire new locks after releasing old locks 

Strict Two Phase Locking => (3) all locks get released together when transacation completes (same as 2PL)

Lock Management
Lock manager => hash table 
    => Key = resource being locked
    => Value = granted set, lock type, waiting queue
        => Granted Set = transactions holding granted locks for the resource 
        => Locktype => S or X 
        => Wait queue => Locks not granted yet due to conflict with existing locks 

Deadlock => Cycle of transactions waiting for locks to be released by each other 

Avoidance => avoid getting into a deadlock 
Priority => Order based on some value (e.g. age = current - start time)

Wait-Die: if T_i has higher priority, T_i waits for T_j; else T_i aborts
Wound-Wait if T_i has higher priority, T_j aborts; else T_i waits 

Detection => Avoidance can still lead to deadlocks 
    => Need to abort one transaction within deadlock to continue 

Waits-for Graph => deadlock detection 
(1) 1 node per transaction
(2) Edge from T_i to T_j if
    (2i) T_j holds lock on resource X 
    (2ii) T_i tries to acquire lock on resource X, 
    but can't acquire because T_j must release lock on X before T_i can acquire it 
Cycle (Deadlock) => Shoot one transaction in cycle, to abort transaction and stop deadlock 

Lock Granularity 
Lock => implicitly places that lock on all it's children 

IS Lock => Intent to place S lock on children 
IX Lock => Intent ot place X lock on children 
SIX Lock => IX Lock + S Lock 

Common Questions: 
1. Determine if transacation has deadlock?
2. True False of ACID properties

# Note 13 - Recovery

Motivation
    - Durability => if transaction commits, transaction never lost
    - Atomiticty => all or none of operations persist
                => no intermediate states ever saved

Force Policy
    - Save modified pages to disk before transactions commits 
        => ensures durability, but to many unceccsary writes 

No Force Policy
    - Only save page to disk before page gets evicted from buffer pool 
        - handled with redos during recovery  

No Steal Policy
    - Pages can't be evicted from memory (i.e. saved to disk), until transaction commits
        => ensures atomiticity, but requires us too many pages in memory
            - doesn't leave database in intermediate state, since stores temp logs in memory 

Steal Policy
    - Allows pages to write to disk before transaction finishes
        - handled with undos during recovery 

Steal, No Force Policy 
    - Allows to reduce total writes => better performance than force, no steal
    - However doesn't ensure atomoticity and durabilty => possible with undo and redo in recovery 

Logging => sequence of log records of operations done on database

UDPATE log record => Write Operation =>  (e.g. SQL insert, delete, update)

COMMIT => transaction starting commit process

ABORT => transcation starting aborting process

END => transaction finished (finished committing or aborting)

Write Ahead Logging (WAL) =>  requirements when we write to disk
    => logs of an action written to disk, before actions themselves written to disk 

1. Logs write to disk before corresponding data pages write to disk 
2. All logs writen to disk when transaction commits 

LSN => Log Sequence Number 
PrevLSN => log request storing last operaiotn from same transaction 

flushed => page written to disk and evicted from memory 
flushedLSN => LSN of last log record flushed to disk (fLSN stored in RAM)

pageLSN => LSN of operation that last modified the page 
    => what operations made to disk and what needs to be redone 

Aborting Transacation => gaurantees consistency 
    => due to deadlock, time limit exceeded, system crash 

Steps to Abort transcation  
    => Write ABORT record to log 
    => Start at last operation in log 
    => Undo each operation in transaction 
    => Write CLR record for each undone operation

Compensation Log Record (CLR) => signifies undo operation 
    => Saves previous and new state like UPDATE record 

Recovery Data Structures => Stored in memory 

Transaction Table
    - Stores XID, status, lastLSN
        - (transaction id), (running commiting aborting), (LSN of most recent transaction)

Dirty Page Table 
    - dirty => modified page not flushed to disk yet 
    - pageID, recLSN 

recLSN => first opreation to dirty the page

Undo Logging 
    => Steal + Force Recovery Mechanism
    => Steps 
        - run recovery manager 
        - scan logs, starting at the end
        - attempt to determine if transaction has ended or not 
            - if COMMIT/ABORT T -> mark T as completed
            - if UPDATE T, X, v -> and if T not completed, write X=v to disk, else ignore
            - if START T -> ignore 

Redo Logging 
    => No Steal + No Force Recovery mechanism 
    => Steps 
        - read log from beginning 
        - redo all updates of commited transactions

ARIES 
    => Steal + No Force Recovery Mechanism
    1. Analysis Phase => Reconstruct transaction table and DPT 
    2. Redo Phase => Repeats operations for durability 
    3. Undo Phase => Undo operations that were running mid-crash 

Analysis Phase 
    - Scan through records from beginning
        - If record not END, add to Xact table 
            - Set lastLSN of Xact to LSN of current record 
        - If record COMMIT or ABORT, change corresponding status in Xact table
        - If record UPDATE and page not in DPT, add to DPT and set recLSN to LSN 
        - If record END, remove Xact from Xact Table
    - For any committing Xact, write END to log and remove Xact from Xact table 
    - Any Xact runnings need to aborted, then ABORT record must be logged 
        -> Modify transaction table and last LSN accordingly
    - Xact table and DPT can reflect any point between BEGIN_CHECKPOINT and END_ENDPOINT

note => page evictions cause evicted page's coresponding LSN and page to be flushed to disk 
note => checkpointing begins at start checkpoint and ends at crash 

checkpointing => writes contents of Xact table and DPT to log 

Redo Phase (durability )
    - Start with smallest recLSN in DPT 
    - Redo all UPDATE and CLR operations EXCEPT records with: 
        1. page not in DPT
        2. recLSN > LSN
        3. pageLSN (disk) >= LSN 

Undo Phase (atomicity)
    - starts at end of log and finishes at beginning of log 
    - Undo every UPDATE record with running or aborting status  
    - writes CLR record to log for each UPDATE record undone (that is running or aborting)
        => Skip commiting or ended transaction's log records 
        => Won't undo operation that is already undone 
        => retrieve LSN from CLR's after abort, and skip them when rolling back
            => CLR records use undoNextLSN that map to prevLSN 
    - Write END record to log after all operations undone 

Code Version
    - Upload Transacation table's lastLSN value into PQ 
    - Undo only Updates associated with transactions in transcation table 

undoNextLSN => stores LSN of next operation to be undone for transaction 
    
# Note 14 - DB Design

Functional Dependencies
- schema redudancy => wasted storage
- functional dependency (fd) => "X determines Y in relation R"
- primay keys => special case of FD 
- superkey => set of columns that determine all columns in table 
- candidate key => set of columns that determine all columns in table
    => however, if you remove one column from CK, then CK no longer a super key (SK)

- set, subsets, closure, monotonic 
    - Clousure of F => F + s.t. set of all FD implied by F
        => full set of relationships that can be determined by known FDs 


- BCNF 
    - Given relation R and set of FD's F
        - For every X -> A: 
            - (1) A subset of X (i.e. X->A is a trivial FD)
            - (2) X is superkey of R (i.e. X+ = R)  

- Decomposition Algo
    - if r in R not in BCNF form and r > 2 attributes 
        - select FD f: X -> A s.t. X, A in r 
        - R_1 = X+ and R_2 = X union (r - X+)
        - Remove r from R 
        - Add R_1 and R_2 to R
        - Recompute F as FDs over all r's in R 
    - repeat until start conditions breaks 


- Lossless Decomp 
    - cannot reconstruct R from decomposed r's 
    

# Note 15 - Parallel Query Processing 

Parallel Query Processing => Query runs on mulitple machines in parallel 

Shared memory => every CPU shares memory and disk 

Shared disk => every CPU has own memory, but shares disk 

Shared nothing => every CPU has own memory and disk, never waits for resources 

(1) Inter-query paralleism => distribute different queries over multiple machines
(2) Intra-query parallelism => distribute parts of single query over multiple machines 
    (a) Inter-operator => distribute different operators over multiple machines 
        (i) Pipeline paralleism => records are piped to parent operation
            => child operates on new record, while parent processes on old record 
        (ii) Bushytree parallelism 
            => different branches of query operation tree are processed on different machines 
    (b) Intra-operator => distribute single operator over multiple machines 

Partitioning 
(1) Sharding => each record has unique machine
(2) Replicaiton => same record can be stored on multiple machines 

Partitioning Scheme => allocation of records to machines 
(1) Range Partitioning => intervals of records are mapped to machine
(2) Hash Partitioning => record mapped to machine by hash function output
(3) Round Robin Partitioning => records added to machine one by one (think modulus) 

Network Cost => data set over network to perform operation 

Parallel Algos 

(1) Parallel Sorting
Passes = 1 + ceil(1 + log_(B-1)(ceil(N/mB)))
=> 1 pass to partition across machines
=> ceil(1 + log_(B-1)(ceil(N/mB))) passes to sort 

(2) Parallel Hashing
Passes => Same as Parallel Sorting

(3) Parallel Sort Merge Join
(i) Range Partition using same ranges on join column
(ii) Perform local sort merge join on each machine 
Total Passes => (1 pass/table to partition accross machines) + (passes to sort R) + (passes to sort S) + (1 merge sort pass over both tables)
sort(R) = ceil(1 + log_(B-2)(ceil(R/mB))) s.t. m = machines, B = buffer pages, R = sort pages

(4) Parallel Grace Hash Join
(i) Hash parition each table using same hash function on join column
(ii) Perform Local Grace Hash Join on each machine 

(5) Broadcast Join
=> send copies of smaller relation to every machine thas has a partition of the larger dataset

(6)Symmetric Hash Join 
    => Requires both table's partitions to fit in memory 
(i) Build two hash tables, one for each table in join 
(iia) Record from R arrives, probe S's hash table for all matches
(iib) Record from S arrives, probe R's hash table for all matcesh 
(iii) Add record to original hash table (R -> R or S -> S) after probing done   

Heirarchical Aggregation => Parallelize aggregation functions
(1) Count: Individually count on machines, send to coordinator who sums everything 
(2) Average: Invidually count and sum values, send to coordinator who sums the sums and divides them  

Common Questions: 
1. What is the network cost of this parallel join? 
2. What is the IO cost of this parallel join? 
3. How many passes does this paralle join use? 


# Note 16 - Distributed Transactions

Deadlock 
	- draw waits-for graphs on top of each other
	- if cycle found in union of graphs, then deadlock 

Two Phase Commit (2PC)
- All nodes agree on a single action 
- Coordinator nodes manages votes and consensus 

Preparation Phase
	(1) Coordinator sends prepare messages to particpations for future commit or abort
    (2) Participant generate prepare or abort record, then flush record to disk 
    (3) Participant to coordinator yes if prepare flushed, or no is abort flushed 
    (4) Coordinator generates commit if unanimous yes vote 

Commit/Abort Phase
	(1) Coordinator flushes, then sends to every participant, result of yes/no vote as commit/abort based on flushed record
    (2) Participants generate corresponding commit or abort record and flush to disk 
    (3) Participants send ACK (acknowledge) message to coordinator
    (4) Coordinator generates End record after recieving all ACK's and flushes to disk in the future 

Distributed Recovery (2PC)
=> Possible Failure
	- Participant recovers, sees no prepare record => aborts locally, doesn't sent any votes 
	- Participant recovers, sees prepare record => asks coordinator about commit, resumes phase 2 after recieving record 
	- Participant recovers, sees commit record => send ACK to coordinator
	- Coordinator recovers, sees no commit record => aborts locally, sends no messages and responds to status inquiry with abort transaction record 
	- Coordinator recovers, sees commit record => rerun phase 2, by sending out ACK again 
	- Coordinator recovers, sees end record => nothing to do, because process is finished 


Distributed Recovery (2PC) with Presumed Abort (Optimization)
Presumed Abort => both coordinator and participant understand that no log records means aborts
    => abort records never need to be flushed 
=> Possible Situations
	- Participant recovers, sees no phase 1 abort record => aborts locally, no messages sent out
	- Participant recovers, sees phase 1 abort record  => aborts locally, no messages sent out (coordinator times out and assumes abort)
	- Participant recovers, sees phase 2 abort record => aborts locally, no messages sent out (coordinator times out and assumes no ack as abort)
	- Coordinator recovers, sees no abort record => aborts locally, no messages sent out & respond to status inquiry as abort 
	- Coordinator recovers, sees abort record => aborts locally, no messages sent out
    - Participant/Coordinator  recovers, sees commit => follow 2PC

Commit Record Recovery: 2PC == 2PC w/ Presumed Abort 


# N17 - NoSQL [IN PROGRESS]

OLTP - online transaction processing
OLAP - online analytical processing

Two-tier architecture 
Three-tier architecture 

Scaling Databases
=> Partitioning
=> Replication
=> CAP Theorem 

NoSQL Datamodel 
=> Key-value Stores
=> Wide-column Stores 
=> Document Stores 

Document vs. Relational Models
JSON => webserver and webbrowser communication via javascript 
-(1) object (2)array (3)atomic 
-self-describing -> schema within a schema

JSON vs. Relational 

Conversion

Querying Semistructure Data 

# Note 18 - Spark [IN PROGRESS]

N/A 

# Real World 

## Postgres SQL 