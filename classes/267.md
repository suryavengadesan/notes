# CS 267
https://sites.google.com/lbl.gov/cs267-spr2022/ 

## Lecture 1

Compute prime factor for billion numbers 

Divide Problem up

## Lecture 2 - Memory Heirarchies and Matrix Multiplication 

- Single Processors 
	- 

- Idealized Uniprocessor Model 
	- 

- Compilers and Assembly Code 
	- manages memory and registers 
		- uses graph coloring algorithm
	- JIT compilers don't use coloring algo 

- Compiler Performs Optimizations
	- Unrolls Loops
	- Fuses Loops
	- Interchanges Loops
	- Eliminates Dead Code
	- Reorders instructions to improve register reuse
	- Strength Reduction

- Realistic Uniprocessor Model 
	
- Latency and Bandwith 

## Lecture 4 - Shared Memory Parallelism

- Shared Memory 
	- program = collection of threads of control

- Synchronize shared variables 

- POSIX threads 
	- PThreads = posix threading interface 

- Parallelism in loops

- Data race 
	- Multiple threads of exection access same variable 
		- At least one of them does a write 
	- Createa a mutex then deallocate a mutex 

- openMP 
	- open specification of multi processing 

- OpenMP basic defintions 
	- /# pragma omp parallel 
	- fork join parallelism 
		- master thread spawns team of threads
		- parallelism incrementally added until performance goals met 
	- Amdhal's Law 
	- Serial pi program 

- Basic Shared Memory Architecture 
	- Caches reduce average latency 
	- Cache Coherence Problem
		- How can modifications in one cache be updated to all caches 
		- Understand write back caches
	- False sharing 

# Lecture 8 - Data Parallel Algorithms

- Parallel Machine and Programming 
	- Shared Memory
	- Distributed Memory
	- Single Instruction Multiple Data

- Data Parallelism
	- perform same operation on multiple values
		- reductions, broadcast, scan
	- examples
		- SIMD
		- CUDA, GPUs
		- MapReduce
		- MPI Collectives

- Unary Operations
	- same operation applied to all element of array
- Binary Operation
	- operations applied to pairs 
- Broadcast
	- fill value into all elements in array

- Memory Operations
	- Array assignemnts work if same array same shape
		- Stride
			- non-continous memory 
		- Gather
			- map array values to specific indices
		- Scatter
			- map array indices to specific values
		- Mask 
			- perfom operation at 1
			- don't perform operation at 0 
		- Reduce 
			- produce scalar from vector (array)
		- Scans
			- fill array with partial reductions
				- i.e. prefix sum 
			- inclusive scan
			- exclusive scan 
- Idealized Hardware 
	- SIMD
		- large numer of tiny processors
		- single control processor issues instructions

- Ideal Model 
	- Machine
		- unbounded number of processors
		- control overhead is free
		- communication is free
	- Cost (complexity)
		- define lower bound on time 
			- i.e. span or depth

- Processor Tree
	- Broadcast 
		- map 1 value to n processors with logn span
	- Reduction
		- map n values to 1 with logn span 
	- utilizes associativity of +, *, min, max

- [TODO] 

# Lecture 9 - Distributed Memory Machines and Programming

- Top 500 supercomputers 
	- benchmarks linear computation 
	- cluster = distributed memory machines 

- Historical perspective
	- distributed memory machines
		- collection of micropressors
		- bi-directional queues between nearest neighbors
	- messages forwarded by processors on path 
		- store and forward networking
		- wormhole routing (today)
	- emphasis on topology
		- minimize message hops
	
- Network Analogy -> Streets
	- Link = street
	- Switch = intersection
	- Distances (hops) = blocks
	- Routing Algo = travel plan 

- Properties
	- Latency
		- street -> time for one car
	- Bandwitdth
	 	- street -> number of cars

- 

# Lecture 10 - Advanced MPI and Collective Communication Algorithms 

- Options 
	- Gloo
	- Mpi
	- Nccl

- Collective Data Movement
- "all" send to all processes 
- "v" signifies a vector of inputs of variable input
	- Broadcast 
	- Scatter 
	- Gather 
	- Reduce (Group to 1)
	- Scan (Prefix Sum)

- Inverse Operations 
	- Scatter <-> Gather 

- Algorithms exist to gaurantee safety 

- SUMMA Algorithm 
	- Scalable Universal Matrix Multiply

- Thread and MPI 
	- MPI_Thread_Multiple 
		- Ordering
		- Blocking

- One-sided Communication 

# Resources 
- Parallel Computing Courses
	- CS267 Applications of Parallel Computers
		- Spring 2023
			- https://sites.google.com/lbl.gov/cs267-spr2023
		- Spring 2022
			- https://sites.google.com/lbl.gov/cs267-spr2022/ 
	- CS315B Parallel Programming: https://web.stanford.edu/class/cs315b/#information
	- CS148 Parallel Computing: https://gfxcourses.stanford.edu/cs149/fall22 

# MPI

- Communicator 
	- Send Method 
		- Arguments 
			- Object - data to be sent
			- Destination - which process rank to send data
			- Tag - distinguish a message 
	- Receive Method 
		- Argument 
			- Object 
			- Destinatoin 
			- Tag
			- Status 
	- Get Rank Method 
		- Returns the current rank
	- Get Size Method 
		- Return total number of processes 
		 
## mpi4py

# SLURM